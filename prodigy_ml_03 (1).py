# -*- coding: utf-8 -*-
"""Prodigy_ML_03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Rb5MetUofJksfSY5FmD8b-xzoSw99qv

# Task 3
"""

import os
from pathlib import Path
from typing import List, Tuple

import joblib
import numpy as np
from PIL import Image
from sklearn.decomposition import PCA
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt
from skimage.feature import hog


TRAIN_DIR = "./data/train"   # path to extracted Kaggle train set
IMG_SIZE = 128               # resize dimension
HIST_BINS = 32               # RGB histogram bins per channel
MAX_PER_CLASS = 2000         # limit per class (use lower for speed, higher for accuracy)
TEST_SIZE = 0.2              # validation split
RANDOM_STATE = 42
USE_PCA = True
PCA_COMPONENTS = 256
MODEL_OUT = "svm_cats_dogs.joblib"
CACHE_FEATURES = True


# ==============================
# Helper Functions
# ==============================

def list_image_paths(train_dir: str, max_per_class: int = None) -> Tuple[List[str], List[int]]:
    """
    Collect cat and dog image paths with labels (0=cat, 1=dog).
    """
    train_dir = Path(train_dir)
    cat_paths = sorted([str(p) for p in train_dir.glob("cat*.jpg")])
    dog_paths = sorted([str(p) for p in train_dir.glob("dog*.jpg")])

    if max_per_class:
        cat_paths = cat_paths[:max_per_class]
        dog_paths = dog_paths[:max_per_class]

    X_paths = cat_paths + dog_paths
    y = [0] * len(cat_paths) + [1] * len(dog_paths)
    return X_paths, y


def extract_features(img_path: str, img_size: int = 128, hist_bins: int = 32) -> np.ndarray:
    """
    Extract HOG features + RGB histogram features.
    """
    with Image.open(img_path) as im:
        im = im.convert("RGB")
        im = im.resize((img_size, img_size))

        # Color histogram
        arr = np.array(im)
        feats_hist = []
        for c in range(3):
            hist, _ = np.histogram(arr[:, :, c], bins=hist_bins, range=(0, 255), density=True)
            feats_hist.append(hist.astype(np.float32))
        feats_hist = np.concatenate(feats_hist)

        # HOG (on grayscale)
        gray = im.convert("L")
        gray = np.array(gray)
        feats_hog = hog(
            gray,
            orientations=9,
            pixels_per_cell=(8, 8),
            cells_per_block=(2, 2),
            block_norm="L2-Hys",
            transform_sqrt=True,
            feature_vector=True,
        ).astype(np.float32)

        return np.concatenate([feats_hog, feats_hist])


def build_feature_matrix(paths: List[str], img_size: int, hist_bins: int, cache_file: str = None) -> np.ndarray:
    """
    Extract features for all images, cache if needed.
    """
    if cache_file and os.path.exists(cache_file):
        data = np.load(cache_file)
        return data["X"]

    X = []
    for i, p in enumerate(paths, 1):
        try:
            feats = extract_features(p, img_size=img_size, hist_bins=hist_bins)
            X.append(feats)
        except Exception as e:
            print(f"[WARN] Skipping {p}: {e}")

        if i % 500 == 0:
            print(f"Processed {i}/{len(paths)} images...")

    X = np.vstack(X).astype(np.float32)
    if cache_file:
        np.savez_compressed(cache_file, X=X)
    return X


# ==============================
# Main Training Workflow
# ==============================

print("==> Loading dataset...")
paths, labels = list_image_paths(TRAIN_DIR, max_per_class=MAX_PER_CLASS)
y = np.array(labels, dtype=np.int64)

cache_file = None
if CACHE_FEATURES:
    cache_file = f"features_{IMG_SIZE}_{HIST_BINS}_{MAX_PER_CLASS or 'all'}.npz"

print("==> Extracting features...")
X = build_feature_matrix(paths, img_size=IMG_SIZE, hist_bins=HIST_BINS, cache_file=cache_file)

print(f"Feature matrix: {X.shape}, Labels: {y.shape}")

print("==> Splitting train/validation...")
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

steps = [("scaler", StandardScaler())]
if USE_PCA:
    steps.append(("pca", PCA(n_components=PCA_COMPONENTS, random_state=RANDOM_STATE)))
steps.append(("svm", LinearSVC(C=1.0, max_iter=10000, dual=True, random_state=RANDOM_STATE)))

clf = Pipeline(steps)

print("==> Training SVM...")
clf.fit(X_train, y_train)

print("==> Evaluating...")
y_pred = clf.predict(X_val)
acc = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {acc:.4f}")
print("\nClassification Report:")
print(classification_report(y_val, y_pred, target_names=["cat", "dog"]))

cm = confusion_matrix(y_val, y_pred, labels=[0, 1])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["cat", "dog"])
disp.plot(values_format="d")
plt.title("Cats vs Dogs - Confusion Matrix")
plt.show()

print(f"==> Saving model to {MODEL_OUT}")
joblib.dump({"pipeline": clf, "img_size": IMG_SIZE, "hist_bins": HIST_BINS, "use_pca": USE_PCA}, MODEL_OUT)
print("Done âœ…")

